pipeline {
    agent any  // Jenkins 파이프라인이 실행될 에이전트를 지정
    triggers {
        GenericTrigger(
            genericVariables: [
                [key: 'USER_NAME', value: '$.pull_request.user.login', expressionType: 'JSONPath'],
                [key: 'IF_MERGED', value: '$.pull_request.merged', expressionType: 'JSONPath'],
                [key: 'BASE_BRANCH', value: '$.pull_request.base.ref', expressionType: 'JSONPath'],
                [key: 'LABEL', value: '$.pull_request.labels[*].name', expressionType: 'JSONPath']
            ],
            causeString: 'Triggered by GitHub Pull Request by ${USER_NAME}',
            token: 'nowdoboss-data-processing',
            printContributedVariables: false,
            printPostContent: false,
            regexpFilterText: '$IF_MERGED $BASE_BRANCH $LABEL',
            regexpFilterExpression: '(?=.*true)(?=.*develop)(?=.*Data-Processing)'
        )
    }

    stages {
        stage('Setup') {
            steps {
                script {
                    currentBuild.description = "Merge requested by: ${env.USER_NAME}"
                }
            }
        }

        stage('Transfer, Deploy, and Build Hadoop and Spark and Airflow') {
            steps {
                script {
                    sshPublisher(
                        publishers: [
                            sshPublisherDesc(
                                configName: 'Sub Server SSH', // Sub Server 설정 이름
                                transfers: [
                                    sshTransfer(
                                        sourceFiles: 'CICD/DataProcessing/**/*', // Sub Server로 전송할 파일
                                        remoteDirectory: 'develop/infra/jenkins', // 작업 디렉터리
                                        execCommand: '''
                                            cd develop/infra/jenkins/CICD/DataProcessing

                                            echo "=== Hadoop + Spark master/worker 상태 확인 ==="

                                            # 마스터
                                            isHadoopRunning_master1=$(docker ps --filter name=master1 --filter status=running --format '{{.Names}}')

                                            # 워커들
                                            isHadoopRunning_worker1=$(docker ps --filter name=worker1 --filter status=running --format '{{.Names}}')
                                            isHadoopRunning_worker2=$(docker ps --filter name=worker2 --filter status=running --format '{{.Names}}')
                                            isHadoopRunning_worker3=$(docker ps --filter name=worker3 --filter status=running --format '{{.Names}}')

                                            if [ -z "$isHadoopRunning_master1" ] || [ -z "$isHadoopRunning_worker1" ] || [ -z "$isHadoopRunning_worker2" ] || [ -z "$isHadoopRunning_worker3" ]; then
                                                echo "하둡 + 스파크 클러스터 빌드중..."
                                                docker-compose -f docker-compose-hadoop-spark.yml --env-file data-processing-env/.env-data-processing up --build -d
                                            else
                                                echo "하둡 스파크 클러스터가 이미 실행중입니다!"
                                            fi

                                            echo "=== Airflow container 상태 확인 ==="

                                            # Airflow Webserver + Scheduler 검사
                                            isAirflowRunning_webserver=$(docker ps --filter name=airflow-webserver --filter status=running --format '{{.Names}}')
                                            isAirflowRunning_scheduler=$(docker ps --filter name=airflow-scheduler --filter status=running --format '{{.Names}}')

                                            if [ -z "$isAirflowRunning_webserver" ] || [ -z "$isAirflowRunning_scheduler" ]; then
                                                echo "Building Airflow image..."
                                                docker-compose -f docker-compose-airflow.yml --env-file data-processing-env/.env-airflow build

                                                echo "Running Airflow init container (ephemeral)..."
                                                docker-compose -f docker-compose-airflow.yml --env-file data-processing-env/.env-airflow run --rm airflow-init
                                                
                                                echo "Starting Airflow scheduler & webserver..."
                                                docker-compose -f docker-compose-airflow.yml --env-file data-processing-env/.env-airflow up -d airflow-scheduler airflow-webserver

                                            else
                                                echo "Airflow가 이미 실행중입니다!"
                                            fi


                                        '''
                                    )
                                ],
                                verbose: true // 상세 로그 활성화
                            )
                        ]
                    )
                }
            }
        }
    }
}
